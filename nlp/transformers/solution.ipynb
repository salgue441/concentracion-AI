{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b7905f-a070-4ffe-abfc-67fbcd2adaa9",
   "metadata": {},
   "source": [
    "## TC3007C\n",
    "\n",
    "## Deep Learning\n",
    "\n",
    "## Transformers\n",
    "\n",
    "### Team Members\n",
    "\n",
    "- [Carlos Salguero](https://github.com/salgue441)\n",
    "- [Diego Perdomo](https://github.com/DiegoPerdomoS)\n",
    "- [Arturo Rendón](https://github.com/00sen)\n",
    "- [José Riosmena](https://github.com/Riosmena)\n",
    "- [Dafne Fernández](https://github.com/Dafne224)\n",
    "\n",
    "#### Activity 3: Implementing a Translator\n",
    "\n",
    "- Objective\n",
    "\n",
    "To understand the Transformer Architecture by Implementing a translator.\n",
    "\n",
    "- Instructions\n",
    "\n",
    "  This activity requires submission in teams. While teamwork is encouraged, each member is expected to contribute individually to the assignment. The final submission should feature the best arguments and solutions from each team member. Only one person per team needs to submit the completed work, but it is imperative that the names of all team members are listed in a Markdown cell at the very beginning of the notebook (either the first or second cell). Failure to include all team member names will result in the grade being awarded solely to the individual who submitted the assignment, with zero points given to other team members (no exceptions will be made to this rule).\n",
    "\n",
    "  Follow the provided code. The code already implements a transformer from scratch as explained in [this video](https://youtu.be/XefFj4rLHgU)\n",
    "\n",
    "  Since the provided code already implements a simple translator, your job for this assignment is to understand it fully, and document it using pictures, figures, and markdown cells. You should test your translator with at least 10 sentences. The dataset used for this task was obtained from [Tatoeba, a large dataset of sentences and translations](https://tatoeba.org/en/downloads).\n",
    "\n",
    "- Evaluation Criteria\n",
    "\n",
    "  - Code Readability and Comments\n",
    "  - Traning a translator\n",
    "  - Translating at least 10 sentences.\n",
    "\n",
    "- Submission\n",
    "\n",
    "Submit this Jupyter Notebook in canvas with your complete solution, ensuring your code is well-commented and includes Markdown cells that explain your design choices, results, and any challenges you encountered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f54c65",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Script to convert csv to text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f02c0c2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbfaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/eng-spa.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(PATH, sep=\"\\t\", on_bad_lines=\"skip\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef686fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673348bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_spa_cols = df.iloc[:, [1, 3]]\n",
    "eng_spa_cols[\"length\"] = eng_spa_cols.iloc[:, 0].str.len()\n",
    "eng_spa_cols = eng_spa_cols.sort_values(by=\"length\")\n",
    "eng_spa_cols = eng_spa_cols.drop(columns=[\"length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03cdb35",
   "metadata": {},
   "source": [
    "Saving the output file locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787d9408",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "output_file_path = \"output/eng-spa4.txt\"\n",
    "eng_spa_cols.to_csv(output_file_path, sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d468e9a",
   "metadata": {},
   "source": [
    "## Transformer - Attention is all you need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dcf681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcc6201",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2cbd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245cbbe",
   "metadata": {},
   "source": [
    "Max sequence length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6623a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136fbad",
   "metadata": {},
   "source": [
    "## Positional Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional embedding module designed to add positional information\n",
    "    to the input tokens.\n",
    "\n",
    "    Attributes:\n",
    "        pos_embeded_matrix (torch.Tensor): The positional embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_seq_len=MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the PositionalEmbedding module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The input tensor with positional information added\n",
    "        \"\"\"\n",
    "\n",
    "        return x + self.pos_embed_matrix[: x.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011288f",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5725a88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module designed to compute the attention\n",
    "    scores between the query, key, and value tensors.\n",
    "\n",
    "    Attributes:\n",
    "        d_v (int): The dimension of the value tensor\n",
    "        d_k (int): The dimension of the key tensor\n",
    "        num_heads (int): The number of heads\n",
    "        W_q (nn.Linear): The linear projection for the query tensor\n",
    "        W_k (nn.Linear): The linear projection for the key tensor\n",
    "        W_v (nn.Linear): The linear projection for the value tensor\n",
    "        W_o (nn.Linear): The linear projection for the output tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"Embedding size not compatible with num heads\"\n",
    "\n",
    "        # Calculate the dimension of each head\n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Define linear projections for query, key, value, and output\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(\n",
    "        self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask=None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the MultiHeadAttention module.\n",
    "\n",
    "        Args:\n",
    "            Q (torch.Tensor): The query tensor\n",
    "            K (torch.Tensor): The key tensor\n",
    "            V (torch.Tensor): The value tensor\n",
    "            mask (torch.Tensor): The mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        weighted_values = (\n",
    "            weighted_values.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        )\n",
    "\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        return weighted_values, attention\n",
    "\n",
    "    def scale_dot_product(\n",
    "        self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask=None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Dot product attention with scaling and masking.\n",
    "\n",
    "        Args:\n",
    "            Q (torch.Tensor): The query tensor\n",
    "            K (torch.Tensor): The key tensor\n",
    "            V (torch.Tensor): The value tensor\n",
    "            mask (torch.Tensor): The mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The weighted values\n",
    "            torch.Tensor: The attention scores\n",
    "        \"\"\"\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "\n",
    "        return weighted_values, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32db8b2",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feedforward module designed to apply two linear\n",
    "    transformations with a ReLU activation in between.\n",
    "\n",
    "    Attributes:\n",
    "        linear1 (nn.Linear): The first linear transformation\n",
    "        linear2 (nn.Linear): The second linear transformation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the PositionFeedForward module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        return self.linear2(F.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78100bce",
   "metadata": {},
   "source": [
    "## Encoder Sublayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderSubLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoded sublayer module designed to apply multi-head attention\n",
    "    and position-wise feedforward operations.\n",
    "\n",
    "    Attributes:\n",
    "        self_attn (MultiHeadAttention): The multi-head attention module\n",
    "        ffn (PositionFeedForward): The position-wise feedforward module\n",
    "        norm1 (nn.LayerNorm): The first layer normalization module\n",
    "        norm2 (nn.LayerNorm): The second layer normalization module\n",
    "        dropout1 (nn.Dropout): The first dropout module\n",
    "        dropout2 (nn.Dropout): The second dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the EncoderSubLayer module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "            mask (torch.Tensor): The mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.dropout2(self.ffn(x))\n",
    "        return self.norm2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477a407",
   "metadata": {},
   "source": [
    "## Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a6aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module designed to apply multiple EncoderSubLayer modules.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): The list of EncoderSubLayer modules\n",
    "        norm (nn.LayerNorm): The layer normalization module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderSubLayer(d_model, num_heads, d_ff, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Encoder module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "            mask (torch.Tensor): The mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9245cc",
   "metadata": {},
   "source": [
    "## Decoder Sublayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8cc2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderSubLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decode sublayer module designed to apply multi-head attention\n",
    "    and position-wise feedforward operations.\n",
    "\n",
    "    Attributes:\n",
    "        self_attn (MultiHeadAttention): The multi-head attention module\n",
    "        cross_attn (MultiHeadAttention): The multi-head attention module\n",
    "        ffn (PositionFeedForward): The position-wise feedforward module\n",
    "        norm1 (nn.LayerNorm): The first layer normalization module\n",
    "        norm2 (nn.LayerNorm): The second layer normalization module\n",
    "        norm3 (nn.LayerNorm): The third layer normalization module\n",
    "        dropout1 (nn.Dropout): The first dropout module\n",
    "        dropout2 (nn.Dropout): The second dropout module\n",
    "        dropout3 (nn.Dropout): The third\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, encoder_output, target_mask=None, encoder_mask=None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the DecoderSubLayer module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "            encoder_output (torch.Tensor): The encoder output tensor\n",
    "            target_mask (torch.Tensor): The target mask tensor\n",
    "            encoder_mask (torch.Tensor): The encoder mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        encoder_attn, _ = self.cross_attn(\n",
    "            x, encoder_output, encoder_output, encoder_mask\n",
    "        )\n",
    "        x = x + self.dropout2(encoder_attn)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        return self.norm3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a8f32",
   "metadata": {},
   "source": [
    "## Decoder Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3103d45f",
   "metadata": {
    "code_folding": [
     30,
     94
    ]
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module designed to apply multiple DecoderSubLayer modules.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): The list of DecoderSubLayer modules\n",
    "        norm (nn.LayerNorm): The layer normalization module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderSubLayer(d_model, num_heads, d_ff, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, encoder_output, target_mask, encoder_mask\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Decoder module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "            encoder_output (torch.Tensor): The encoder output tensor\n",
    "            target_mask (torch.Tensor): The target mask tensor\n",
    "            encoder_mask (torch.Tensor): The encoder mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b532de",
   "metadata": {},
   "source": [
    "## Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61070162",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer module designed to translate sequences from one language to\n",
    "    another using an encoder and decoder architecture.\n",
    "\n",
    "    Attributes:\n",
    "        encoder_embedding (nn.Embedding): The embedding layer for the encoder\n",
    "        decoder_embedding (nn.Embedding): The embedding layer for the decoder\n",
    "        pos_embedding (PositionalEmbedding): The positional embedding layer\n",
    "        encoder (Encoder): The encoder module\n",
    "        decoder (Decoder): The decoder module\n",
    "        output_layer (nn.Linear): The output layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        d_ff,\n",
    "        num_layers,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        max_len=MAX_SEQ_LEN,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer module.\n",
    "\n",
    "        Args:\n",
    "            source (torch.Tensor): The source tensor\n",
    "            target (torch.Tensor): The target tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "        source = self.encoder_embedding(source) * math.sqrt(\n",
    "            self.encoder_embedding.embedding_dim\n",
    "        )\n",
    "        source = self.pos_embedding(source)\n",
    "\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        target = self.decoder_embedding(target) * math.sqrt(\n",
    "            self.decoder_embedding.embedding_dim\n",
    "        )\n",
    "        target = self.pos_embedding(target)\n",
    "\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        return self.output_layer(output)\n",
    "\n",
    "    def mask(\n",
    "        self, source: torch.Tensor, target: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Masks the source and target tensors to prevent the model from\n",
    "        attending to the padding tokens.\n",
    "\n",
    "        Args:\n",
    "            source (torch.Tensor): The source tensor\n",
    "            target (torch.Tensor): The target tensor\n",
    "\n",
    "        Returns:\n",
    "            List[torch.Tensor]: The source and target masks\n",
    "        \"\"\"\n",
    "\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        size = target.size(1)\n",
    "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()\n",
    "        target_mask = target_mask & no_mask\n",
    "\n",
    "        return source_mask, target_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6b2d4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Simple test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f2cc4",
   "metadata": {},
   "source": [
    "### Sequence parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40581d6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "seq_len_source = 10\n",
    "seq_len_target = 10\n",
    "batch_size = 2\n",
    "input_vocab_size = 50\n",
    "target_vocab_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24939149",
   "metadata": {},
   "source": [
    "### Input & Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2f6d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n",
    "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a43a9",
   "metadata": {},
   "source": [
    "### Model Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259d56a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3732c5",
   "metadata": {},
   "source": [
    "## Model Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4254c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    num_layers,\n",
    "    input_vocab_size,\n",
    "    target_vocab_size,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "source = source.to(device)\n",
    "target = target.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb96380",
   "metadata": {},
   "source": [
    "Computing the model output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71ed8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(source, target)\n",
    "print(f\"ouput.shape {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b2910",
   "metadata": {},
   "source": [
    "## Translator Eng-Spa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dff394",
   "metadata": {},
   "source": [
    "### English File Reading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a7244",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"output/eng-spa4.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0af1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "eng_spa_pairs = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c930226f",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_spa_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f4037",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = [pair[0] for pair in eng_spa_pairs]\n",
    "spa_sentences = [pair[1] for pair in eng_spa_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9e1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eng_sentences[:10])\n",
    "print(spa_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d11478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Function to preprocess a sentence by converting to lowercase, removing special characters,\n",
    "    and adding start and end tokens.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed sentence\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"[á]+\", \"a\", sentence)\n",
    "    sentence = re.sub(r\"[é]+\", \"e\", sentence)\n",
    "    sentence = re.sub(r\"[í]+\", \"i\", sentence)\n",
    "    sentence = re.sub(r\"[ó]+\", \"o\", sentence)\n",
    "    sentence = re.sub(r\"[ú]+\", \"u\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = \"<sos> \" + sentence + \" <eos>\"\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890975f",
   "metadata": {},
   "source": [
    "### Sample Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = \"¿Hola @ cómo estás? 123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ac79c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s1)\n",
    "print(preprocess_sentence(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fc9c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
    "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a3b18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "spa_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97931cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(sentences: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Function to build a vocabulary from a list of sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): containing input sentences\n",
    "\n",
    "    Returns:\n",
    "        word2idx: dict, mapping words to indices\n",
    "        idx2word: dict, mapping indices to words\n",
    "    \"\"\"\n",
    "\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    word_count = Counter(words)\n",
    "\n",
    "    sorted_word_counts = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
    "\n",
    "    word2idx[\"<pad>\"] = 0  # Reserved for padding\n",
    "    word2idx[\"<unk>\"] = 1  # Reserved for unknown words\n",
    "\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c06594",
   "metadata": {},
   "source": [
    "## Building the english & spanish vocabularies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
    "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
    "eng_vocab_size = len(eng_word2idx)\n",
    "spa_vocab_size = len(spa_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eng_vocab_size, spa_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc8786",
   "metadata": {},
   "source": [
    "## EngSpaDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e564017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EngSpaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    English-Spanish dataset class designed to convert sentences into\n",
    "    word indices using the word-to-index dictionaries.\n",
    "\n",
    "    Attributes:\n",
    "        eng_sentences (List[str]): List of English sentences\n",
    "        spa_sentences (List[str]): List of Spanish sentences\n",
    "        eng_word2idx (dict): Dictionary to map English words to indices\n",
    "        spa_word2idx (dict): Dictionary to map Spanish words to indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eng_sentences: List[str],\n",
    "        spa_sentences: List[str],\n",
    "        eng_word2idx: dict,\n",
    "        spa_word2idx: dict,\n",
    "    ):\n",
    "        self.eng_sentences = eng_sentences\n",
    "        self.spa_sentences = spa_sentences\n",
    "        self.eng_word2idx = eng_word2idx\n",
    "        self.spa_word2idx = spa_word2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Computes the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The length of the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.eng_sentences)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Gets an item from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the item to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: The English and Spanish sentence indices\n",
    "        \"\"\"\n",
    "\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        spa_sentence = self.spa_sentences[idx]\n",
    "\n",
    "        eng_idxs = [\n",
    "            self.eng_word2idx.get(word, self.eng_word2idx[\"<unk>\"])\n",
    "            for word in eng_sentence.split()\n",
    "        ]\n",
    "\n",
    "        spa_idxs = [\n",
    "            self.spa_word2idx.get(word, self.spa_word2idx[\"<unk>\"])\n",
    "            for word in spa_sentence.split()\n",
    "        ]\n",
    "\n",
    "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff13ae",
   "metadata": {},
   "source": [
    "## Collate Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b579577b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(\n",
    "    batch: List[Tuple[torch.Tensor, torch.Tensor]]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Function to pad sequences in a batch to the same length.\n",
    "\n",
    "    Args:\n",
    "        batch (List[Tuple[torch.Tensor, torch.Tensor]]): The batch of data\n",
    "\n",
    "    Returns:\n",
    "        eng_batch: tensor, padded English sentences\n",
    "        spa_batch: tensor, padded Spanish sentences\n",
    "    \"\"\"\n",
    "\n",
    "    eng_batch, spa_batch = zip(*batch)\n",
    "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
    "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
    "\n",
    "    eng_batch = torch.nn.utils.rnn.pad_sequence(\n",
    "        eng_batch, batch_first=True, padding_value=0\n",
    "    )\n",
    "\n",
    "    spa_batch = torch.nn.utils.rnn.pad_sequence(\n",
    "        spa_batch, batch_first=True, padding_value=0\n",
    "    )\n",
    "\n",
    "    return eng_batch, spa_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3708be0",
   "metadata": {},
   "source": [
    "## Train Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d514b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_function, optimiser, epochs):\n",
    "    \"\"\"\n",
    "    Training loop for the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): The Transformer model\n",
    "        dataloader (DataLoader): The DataLoader object\n",
    "        loss_function (nn.CrossEntropyLoss): The loss function\n",
    "        optimiser (optim.Adam): The optimiser\n",
    "        epochs (int): The number of epochs\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
    "            eng_batch = eng_batch.to(device)\n",
    "            spa_batch = spa_batch.to(device)\n",
    "\n",
    "            target_input = spa_batch[:, :-1]\n",
    "            target_output = spa_batch[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            output = model(eng_batch, target_input)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "\n",
    "            loss = loss_function(output, target_output)\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb533b",
   "metadata": {},
   "source": [
    "## Data Loader Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379ea72",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4affae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13079c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6802e",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08eef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    input_vocab_size=eng_vocab_size,\n",
    "    target_vocab_size=spa_vocab_size,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1181a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e265e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, dataloader, loss_function, optimiser, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b2bcf",
   "metadata": {},
   "source": [
    "## Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50740746",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence, word2idx):\n",
    "    \"\"\"\n",
    "    Converts a sentence into a list of indices using a word-to-index dictionary.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The sentence to convert.\n",
    "        word2idx (dict): The dictionary mapping words to indices.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of indices corresponding to the words in the sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    return [word2idx.get(word, word2idx[\"<unk>\"]) for word in sentence.split()]\n",
    "\n",
    "\n",
    "def indices_to_sentence(indices, idx2word):\n",
    "    \"\"\"\n",
    "    Converts a list of indices back into a sentence using an index-to-word dictionary.\n",
    "\n",
    "    Args:\n",
    "        indices (list): The list of indices to convert.\n",
    "        idx2word (dict): The dictionary mapping indices to words.\n",
    "\n",
    "    Returns:\n",
    "        str: The sentence corresponding to the indices.\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join(\n",
    "        [\n",
    "            idx2word[idx]\n",
    "            for idx in indices\n",
    "            if idx in idx2word and idx2word[idx] != \"<pad>\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def translate_sentence(\n",
    "    model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=\"cpu\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translates a sentence from English to Spanish using the trained Transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained Transformer model.\n",
    "        sentence (str): The English sentence to translate.\n",
    "        eng_word2idx (dict): The dictionary mapping English words to indices.\n",
    "        spa_idx2word (dict): The dictionary mapping Spanish indices to words.\n",
    "        max_len (int): The maximum length of the translated sentence.\n",
    "        device (str): The device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        str: The translated Spanish sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    input_indices = sentence_to_indices(sentence, eng_word2idx)\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    tgt_indices = [spa_word2idx[\"<sos>\"]]\n",
    "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_tensor, tgt_tensor)\n",
    "\n",
    "            output = output.squeeze(0)\n",
    "\n",
    "            next_token = output.argmax(dim=-1)[-1].item()\n",
    "            tgt_indices.append(next_token)\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "            if next_token == spa_word2idx[\"<eos>\"]:\n",
    "                break\n",
    "\n",
    "    return indices_to_sentence(tgt_indices, spa_idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13954aa5",
   "metadata": {},
   "source": [
    "## Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0db72",
   "metadata": {
    "code_folding": [
     15
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_translations(\n",
    "    model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates translations for a list of sentences using the trained Transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained Transformer model.\n",
    "        sentences (list): A list of sentences to translate.\n",
    "        eng_word2idx (dict): The dictionary mapping English words to indices.\n",
    "        spa_idx2word (dict): The dictionary mapping Spanish indices to words.\n",
    "        max_len (int): The maximum length of the translated sentence.\n",
    "        device (str): The device to run the model on ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        translation = translate_sentence(\n",
    "            model, sentence, eng_word2idx, spa_idx2word, max_len, device\n",
    "        )\n",
    "\n",
    "        print(f\"Input sentence: {sentence}\")\n",
    "        print(f\"Traducción: {translation}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f9e36",
   "metadata": {},
   "source": [
    "## Testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35742176",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am learning artificial intelligence.\",\n",
    "    \"Artificial intelligence is great.\",\n",
    "    \"Good night!\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e703f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06836633",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_translations(\n",
    "    model,\n",
    "    test_sentences,\n",
    "    eng_word2idx,\n",
    "    spa_idx2word,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    device=device,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
