{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b7905f-a070-4ffe-abfc-67fbcd2adaa9",
   "metadata": {},
   "source": [
    "## Neural Machine Translation with Transformers\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "Implementation of a Neural Machine Translation system using the Transformer architecture, focusing on sequence-to-sequence learning for language translation tasks.\n",
    "\n",
    "## üë• Team Members\n",
    "\n",
    "- [Carlos Salguero](https://github.com/salgue441)\n",
    "- [Diego Perdomo](https://github.com/DiegoPerdomoS)\n",
    "- [Arturo Rend√≥n](https://github.com/00sen)\n",
    "- [Jos√© Riosmena](https://github.com/Riosmena)\n",
    "- [Dafne Fern√°ndez](https://github.com/Dafne224)\n",
    "\n",
    "## üéØ Objectives\n",
    "\n",
    "1. Implement and understand the Transformer architecture\n",
    "2. Develop a functional language translation system\n",
    "3. Apply attention mechanisms in sequence-to-sequence learning\n",
    "4. Evaluate translation quality using appropriate metrics\n",
    "\n",
    "## üìö Dataset\n",
    "\n",
    "This project uses the Tatoeba dataset:\n",
    "\n",
    "- Source: Tatoeba, a large dataset of sentences and translations\n",
    "- Type: parallel corpus for language translation\n",
    "- Content: sequence pairs in source and target languages\n",
    "\n",
    "## üìù Evaluation Criteria\n",
    "\n",
    "| Criterion | Weight |\n",
    "| --------- | ------ |\n",
    "| Code quality & documentation | 40% |\n",
    "| Model implementation | 30% |\n",
    "| Translation performance | 30% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f54c65",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Script to convert csv to text file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f02c0c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:45.980233Z",
     "iopub.status.busy": "2024-11-25T00:52:45.979901Z",
     "iopub.status.idle": "2024-11-25T00:52:46.289566Z",
     "shell.execute_reply": "2024-11-25T00:52:46.288905Z",
     "shell.execute_reply.started": "2024-11-25T00:52:45.980206Z"
    },
    "hidden": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6fbfaf8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:46.291252Z",
     "iopub.status.busy": "2024-11-25T00:52:46.290940Z",
     "iopub.status.idle": "2024-11-25T00:52:46.294779Z",
     "shell.execute_reply": "2024-11-25T00:52:46.293976Z",
     "shell.execute_reply.started": "2024-11-25T00:52:46.291227Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PATH = \"/kaggle/input/english-spanish/eng-spa.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887b425f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:46.295991Z",
     "iopub.status.busy": "2024-11-25T00:52:46.295758Z",
     "iopub.status.idle": "2024-11-25T00:52:47.117382Z",
     "shell.execute_reply": "2024-11-25T00:52:47.116444Z",
     "shell.execute_reply.started": "2024-11-25T00:52:46.295968Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 4\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(PATH, sep=\"\\t\", on_bad_lines=\"skip\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef686fb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:47.120004Z",
     "iopub.status.busy": "2024-11-25T00:52:47.119591Z",
     "iopub.status.idle": "2024-11-25T00:52:47.136123Z",
     "shell.execute_reply": "2024-11-25T00:52:47.135292Z",
     "shell.execute_reply.started": "2024-11-25T00:52:47.119966Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1276</th>\n",
       "      <th>Let's try something.</th>\n",
       "      <th>2481</th>\n",
       "      <th>¬°Intentemos algo!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1277</td>\n",
       "      <td>I have to go to sleep.</td>\n",
       "      <td>2482</td>\n",
       "      <td>Tengo que irme a dormir.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1280</td>\n",
       "      <td>Today is June 18th and it is Muiriel's birthday!</td>\n",
       "      <td>2485</td>\n",
       "      <td>¬°Hoy es 18 de junio y es el cumplea√±os de Muir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1280</td>\n",
       "      <td>Today is June 18th and it is Muiriel's birthday!</td>\n",
       "      <td>1130137</td>\n",
       "      <td>¬°Hoy es el 18 de junio y es el cumplea√±os de M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1282</td>\n",
       "      <td>Muiriel is 20 now.</td>\n",
       "      <td>2487</td>\n",
       "      <td>Ahora, Muiriel tiene 20 a√±os.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1282</td>\n",
       "      <td>Muiriel is 20 now.</td>\n",
       "      <td>1130133</td>\n",
       "      <td>Muiriel tiene 20 a√±os ahora.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1276                              Let's try something.     2481  \\\n",
       "0  1277                            I have to go to sleep.     2482   \n",
       "1  1280  Today is June 18th and it is Muiriel's birthday!     2485   \n",
       "2  1280  Today is June 18th and it is Muiriel's birthday!  1130137   \n",
       "3  1282                                Muiriel is 20 now.     2487   \n",
       "4  1282                                Muiriel is 20 now.  1130133   \n",
       "\n",
       "                                   ¬°Intentemos algo!  \n",
       "0                           Tengo que irme a dormir.  \n",
       "1  ¬°Hoy es 18 de junio y es el cumplea√±os de Muir...  \n",
       "2  ¬°Hoy es el 18 de junio y es el cumplea√±os de M...  \n",
       "3                      Ahora, Muiriel tiene 20 a√±os.  \n",
       "4                       Muiriel tiene 20 a√±os ahora.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "673348bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:47.137279Z",
     "iopub.status.busy": "2024-11-25T00:52:47.137030Z",
     "iopub.status.idle": "2024-11-25T00:52:47.271589Z",
     "shell.execute_reply": "2024-11-25T00:52:47.270665Z",
     "shell.execute_reply.started": "2024-11-25T00:52:47.137256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30/4043921570.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eng_spa_cols[\"length\"] = eng_spa_cols.iloc[:, 0].str.len()\n"
     ]
    }
   ],
   "source": [
    "eng_spa_cols = df.iloc[:, [1, 3]]\n",
    "eng_spa_cols[\"length\"] = eng_spa_cols.iloc[:, 0].str.len()\n",
    "eng_spa_cols = eng_spa_cols.sort_values(by=\"length\")\n",
    "eng_spa_cols = eng_spa_cols.drop(columns=[\"length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03cdb35",
   "metadata": {},
   "source": [
    "Saving the output file locally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "787d9408",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:47.273232Z",
     "iopub.status.busy": "2024-11-25T00:52:47.272970Z",
     "iopub.status.idle": "2024-11-25T00:52:47.913031Z",
     "shell.execute_reply": "2024-11-25T00:52:47.912300Z",
     "shell.execute_reply.started": "2024-11-25T00:52:47.273206Z"
    },
    "hidden": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "output_file_path = \"/kaggle/working/eng-spa4.txt\"\n",
    "eng_spa_cols.to_csv(output_file_path, sep=\"\\t\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d468e9a",
   "metadata": {},
   "source": [
    "## Transformer - Attention is all you need\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5dcf681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:47.914168Z",
     "iopub.status.busy": "2024-11-25T00:52:47.913942Z",
     "iopub.status.idle": "2024-11-25T00:52:50.903752Z",
     "shell.execute_reply": "2024-11-25T00:52:50.902988Z",
     "shell.execute_reply.started": "2024-11-25T00:52:47.914146Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbcc6201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:50.905248Z",
     "iopub.status.busy": "2024-11-25T00:52:50.904778Z",
     "iopub.status.idle": "2024-11-25T00:52:50.914456Z",
     "shell.execute_reply": "2024-11-25T00:52:50.913658Z",
     "shell.execute_reply.started": "2024-11-25T00:52:50.905203Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7af90a73b450>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c2cbd17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:50.915843Z",
     "iopub.status.busy": "2024-11-25T00:52:50.915510Z",
     "iopub.status.idle": "2024-11-25T00:52:50.951003Z",
     "shell.execute_reply": "2024-11-25T00:52:50.950150Z",
     "shell.execute_reply.started": "2024-11-25T00:52:50.915819Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245cbbe",
   "metadata": {},
   "source": [
    "Max sequence length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c6623a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:50.953975Z",
     "iopub.status.busy": "2024-11-25T00:52:50.953686Z",
     "iopub.status.idle": "2024-11-25T00:52:50.961205Z",
     "shell.execute_reply": "2024-11-25T00:52:50.960527Z",
     "shell.execute_reply.started": "2024-11-25T00:52:50.953951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136fbad",
   "metadata": {},
   "source": [
    "## Positional Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c460df01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:50.962377Z",
     "iopub.status.busy": "2024-11-25T00:52:50.962135Z",
     "iopub.status.idle": "2024-11-25T00:52:50.974168Z",
     "shell.execute_reply": "2024-11-25T00:52:50.973355Z",
     "shell.execute_reply.started": "2024-11-25T00:52:50.962353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional embedding module designed to add positional information\n",
    "    to the input tokens.\n",
    "\n",
    "    Attributes:\n",
    "        pos_embeded_matrix (torch.Tensor): The positional embedding matrix\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, max_seq_len=MAX_SEQ_LEN):\n",
    "        super().__init__()\n",
    "        self.pos_embed_matrix = torch.zeros(max_seq_len, d_model, device=device)\n",
    "\n",
    "        token_pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        self.pos_embed_matrix[:, 0::2] = torch.sin(token_pos * div_term)\n",
    "        self.pos_embed_matrix[:, 1::2] = torch.cos(token_pos * div_term)\n",
    "        self.pos_embed_matrix = self.pos_embed_matrix.unsqueeze(0).transpose(0, 1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the PositionalEmbedding module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The input tensor with positional information added\n",
    "        \"\"\"\n",
    "\n",
    "        return x + self.pos_embed_matrix[: x.size(0), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0011288f",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5725a88a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:50.976043Z",
     "iopub.status.busy": "2024-11-25T00:52:50.975277Z",
     "iopub.status.idle": "2024-11-25T00:52:50.988109Z",
     "shell.execute_reply": "2024-11-25T00:52:50.987386Z",
     "shell.execute_reply.started": "2024-11-25T00:52:50.976004Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module designed to compute the attention\n",
    "    scores between the query, key, and value tensors.\n",
    "\n",
    "    Attributes:\n",
    "        d_v (int): The dimension of the value tensor\n",
    "        d_k (int): The dimension of the key tensor\n",
    "        num_heads (int): The number of heads\n",
    "        W_q (nn.Linear): The linear projection for the query tensor\n",
    "        W_k (nn.Linear): The linear projection for the key tensor\n",
    "        W_v (nn.Linear): The linear projection for the value tensor\n",
    "        W_o (nn.Linear): The linear projection for the output tensor\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model=512, num_heads=8):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"Embedding size not compatible with num heads\"\n",
    "\n",
    "        # Calculate the dimension of each head\n",
    "        self.d_v = d_model // num_heads\n",
    "        self.d_k = self.d_v\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        # Define linear projections for query, key, value, and output\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(\n",
    "        self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask=None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the MultiHeadAttention module.\n",
    "\n",
    "        Args:\n",
    "            Q (torch.Tensor): The query tensor\n",
    "            K (torch.Tensor): The key tensor\n",
    "            V (torch.Tensor): The value tensor\n",
    "            mask (torch.Tensor): The mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = Q.size(0)\n",
    "\n",
    "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "        weighted_values, attention = self.scale_dot_product(Q, K, V, mask)\n",
    "        weighted_values = (\n",
    "            weighted_values.transpose(1, 2)\n",
    "            .contiguous()\n",
    "            .view(batch_size, -1, self.num_heads * self.d_k)\n",
    "        )\n",
    "\n",
    "        weighted_values = self.W_o(weighted_values)\n",
    "        return weighted_values, attention\n",
    "\n",
    "    def scale_dot_product(\n",
    "        self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, mask=None\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Dot product attention with scaling and masking.\n",
    "\n",
    "        Args:\n",
    "            Q (torch.Tensor): The query tensor\n",
    "            K (torch.Tensor): The key tensor\n",
    "            V (torch.Tensor): The value tensor\n",
    "            mask (torch.Tensor): The mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The weighted values\n",
    "            torch.Tensor: The attention scores\n",
    "        \"\"\"\n",
    "\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "        weighted_values = torch.matmul(attention, V)\n",
    "\n",
    "        return weighted_values, attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32db8b2",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3587ab1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:50.989253Z",
     "iopub.status.busy": "2024-11-25T00:52:50.989021Z",
     "iopub.status.idle": "2024-11-25T00:52:51.003326Z",
     "shell.execute_reply": "2024-11-25T00:52:51.002482Z",
     "shell.execute_reply.started": "2024-11-25T00:52:50.989230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Position-wise feedforward module designed to apply two linear\n",
    "    transformations with a ReLU activation in between.\n",
    "\n",
    "    Attributes:\n",
    "        linear1 (nn.Linear): The first linear transformation\n",
    "        linear2 (nn.Linear): The second linear transformation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the PositionFeedForward module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        return self.linear2(F.relu(self.linear1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78100bce",
   "metadata": {},
   "source": [
    "## Encoder Sublayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9354f636",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:51.004621Z",
     "iopub.status.busy": "2024-11-25T00:52:51.004291Z",
     "iopub.status.idle": "2024-11-25T00:52:51.015778Z",
     "shell.execute_reply": "2024-11-25T00:52:51.014949Z",
     "shell.execute_reply.started": "2024-11-25T00:52:51.004566Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderSubLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoded sublayer module designed to apply multi-head attention\n",
    "    and position-wise feedforward operations.\n",
    "\n",
    "    Attributes:\n",
    "        self_attn (MultiHeadAttention): The multi-head attention module\n",
    "        ffn (PositionFeedForward): The position-wise feedforward module\n",
    "        norm1 (nn.LayerNorm): The first layer normalization module\n",
    "        norm2 (nn.LayerNorm): The second layer normalization module\n",
    "        dropout1 (nn.Dropout): The first dropout module\n",
    "        dropout2 (nn.Dropout): The second dropout\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the EncoderSubLayer module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "            mask (torch.Tensor): The mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        attention_score, _ = self.self_attn(x, x, x, mask)\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.dropout2(self.ffn(x))\n",
    "        return self.norm2(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477a407",
   "metadata": {},
   "source": [
    "## Encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82a6aaf9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:51.017035Z",
     "iopub.status.busy": "2024-11-25T00:52:51.016797Z",
     "iopub.status.idle": "2024-11-25T00:52:51.032380Z",
     "shell.execute_reply": "2024-11-25T00:52:51.031541Z",
     "shell.execute_reply.started": "2024-11-25T00:52:51.017013Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder module designed to apply multiple EncoderSubLayer modules.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): The list of EncoderSubLayer modules\n",
    "        norm (nn.LayerNorm): The layer normalization module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderSubLayer(d_model, num_heads, d_ff, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask=None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Encoder module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "            mask (torch.Tensor): The mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9245cc",
   "metadata": {},
   "source": [
    "## Decoder Sublayer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f8cc2fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:51.033834Z",
     "iopub.status.busy": "2024-11-25T00:52:51.033546Z",
     "iopub.status.idle": "2024-11-25T00:52:51.046289Z",
     "shell.execute_reply": "2024-11-25T00:52:51.045660Z",
     "shell.execute_reply.started": "2024-11-25T00:52:51.033793Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderSubLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Decode sublayer module designed to apply multi-head attention\n",
    "    and position-wise feedforward operations.\n",
    "\n",
    "    Attributes:\n",
    "        self_attn (MultiHeadAttention): The multi-head attention module\n",
    "        cross_attn (MultiHeadAttention): The multi-head attention module\n",
    "        ffn (PositionFeedForward): The position-wise feedforward module\n",
    "        norm1 (nn.LayerNorm): The first layer normalization module\n",
    "        norm2 (nn.LayerNorm): The second layer normalization module\n",
    "        norm3 (nn.LayerNorm): The third layer normalization module\n",
    "        dropout1 (nn.Dropout): The first dropout module\n",
    "        dropout2 (nn.Dropout): The second dropout module\n",
    "        dropout3 (nn.Dropout): The third\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.dropout3 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, encoder_output, target_mask=None, encoder_mask=None\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the DecoderSubLayer module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "            encoder_output (torch.Tensor): The encoder output tensor\n",
    "            target_mask (torch.Tensor): The target mask tensor\n",
    "            encoder_mask (torch.Tensor): The encoder mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        attention_score, _ = self.self_attn(x, x, x, target_mask)\n",
    "        x = x + self.dropout1(attention_score)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        encoder_attn, _ = self.cross_attn(\n",
    "            x, encoder_output, encoder_output, encoder_mask\n",
    "        )\n",
    "        x = x + self.dropout2(encoder_attn)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + self.dropout3(ff_output)\n",
    "        return self.norm3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4a8f32",
   "metadata": {},
   "source": [
    "## Decoder Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3103d45f",
   "metadata": {
    "code_folding": [
     30,
     94
    ],
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:51.047551Z",
     "iopub.status.busy": "2024-11-25T00:52:51.047240Z",
     "iopub.status.idle": "2024-11-25T00:52:51.060042Z",
     "shell.execute_reply": "2024-11-25T00:52:51.059316Z",
     "shell.execute_reply.started": "2024-11-25T00:52:51.047515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder module designed to apply multiple DecoderSubLayer modules.\n",
    "\n",
    "    Attributes:\n",
    "        layers (nn.ModuleList): The list of DecoderSubLayer modules\n",
    "        norm (nn.LayerNorm): The layer normalization module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderSubLayer(d_model, num_heads, d_ff, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, encoder_output, target_mask, encoder_mask\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Decoder module.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor\n",
    "            encoder_output (torch.Tensor): The encoder output tensor\n",
    "            target_mask (torch.Tensor): The target mask tensor\n",
    "            encoder_mask (torch.Tensor): The encoder mask tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, encoder_output, target_mask, encoder_mask)\n",
    "\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b532de",
   "metadata": {},
   "source": [
    "## Transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61070162",
   "metadata": {
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:51.061342Z",
     "iopub.status.busy": "2024-11-25T00:52:51.061096Z",
     "iopub.status.idle": "2024-11-25T00:52:51.073260Z",
     "shell.execute_reply": "2024-11-25T00:52:51.072562Z",
     "shell.execute_reply.started": "2024-11-25T00:52:51.061320Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer module designed to translate sequences from one language to\n",
    "    another using an encoder and decoder architecture.\n",
    "\n",
    "    Attributes:\n",
    "        encoder_embedding (nn.Embedding): The embedding layer for the encoder\n",
    "        decoder_embedding (nn.Embedding): The embedding layer for the decoder\n",
    "        pos_embedding (PositionalEmbedding): The positional embedding layer\n",
    "        encoder (Encoder): The encoder module\n",
    "        decoder (Decoder): The decoder module\n",
    "        output_layer (nn.Linear): The output layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        num_heads,\n",
    "        d_ff,\n",
    "        num_layers,\n",
    "        input_vocab_size,\n",
    "        target_vocab_size,\n",
    "        max_len=MAX_SEQ_LEN,\n",
    "        dropout=0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_embedding = PositionalEmbedding(d_model, max_len)\n",
    "        self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        self.output_layer = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def forward(self, source: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer module.\n",
    "\n",
    "        Args:\n",
    "            source (torch.Tensor): The source tensor\n",
    "            target (torch.Tensor): The target tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor\n",
    "        \"\"\"\n",
    "\n",
    "        source_mask, target_mask = self.mask(source, target)\n",
    "        source = self.encoder_embedding(source) * math.sqrt(\n",
    "            self.encoder_embedding.embedding_dim\n",
    "        )\n",
    "        source = self.pos_embedding(source)\n",
    "\n",
    "        encoder_output = self.encoder(source, source_mask)\n",
    "        target = self.decoder_embedding(target) * math.sqrt(\n",
    "            self.decoder_embedding.embedding_dim\n",
    "        )\n",
    "        target = self.pos_embedding(target)\n",
    "\n",
    "        output = self.decoder(target, encoder_output, target_mask, source_mask)\n",
    "        return self.output_layer(output)\n",
    "\n",
    "    def mask(\n",
    "        self, source: torch.Tensor, target: torch.Tensor\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Masks the source and target tensors to prevent the model from\n",
    "        attending to the padding tokens.\n",
    "\n",
    "        Args:\n",
    "            source (torch.Tensor): The source tensor\n",
    "            target (torch.Tensor): The target tensor\n",
    "\n",
    "        Returns:\n",
    "            List[torch.Tensor]: The source and target masks\n",
    "        \"\"\"\n",
    "\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2)\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "        size = target.size(1)\n",
    "        no_mask = torch.tril(torch.ones((1, size, size), device=device)).bool()\n",
    "        target_mask = target_mask & no_mask\n",
    "\n",
    "        return source_mask, target_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6b2d4",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Simple test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28f2cc4",
   "metadata": {},
   "source": [
    "### Sequence parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d40581d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:51.074470Z",
     "iopub.status.busy": "2024-11-25T00:52:51.074157Z",
     "iopub.status.idle": "2024-11-25T00:52:51.092887Z",
     "shell.execute_reply": "2024-11-25T00:52:51.092152Z",
     "shell.execute_reply.started": "2024-11-25T00:52:51.074433Z"
    },
    "hidden": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "seq_len_source = 10\n",
    "seq_len_target = 10\n",
    "batch_size = 2\n",
    "input_vocab_size = 50\n",
    "target_vocab_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24939149",
   "metadata": {},
   "source": [
    "### Input & Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f2f6d73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:51.094165Z",
     "iopub.status.busy": "2024-11-25T00:52:51.093847Z",
     "iopub.status.idle": "2024-11-25T00:52:51.119205Z",
     "shell.execute_reply": "2024-11-25T00:52:51.118644Z",
     "shell.execute_reply.started": "2024-11-25T00:52:51.094130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "source = torch.randint(1, input_vocab_size, (batch_size, seq_len_source))\n",
    "target = torch.randint(1, target_vocab_size, (batch_size, seq_len_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502a43a9",
   "metadata": {},
   "source": [
    "### Model Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "259d56a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:51.120417Z",
     "iopub.status.busy": "2024-11-25T00:52:51.120090Z",
     "iopub.status.idle": "2024-11-25T00:52:51.124142Z",
     "shell.execute_reply": "2024-11-25T00:52:51.123287Z",
     "shell.execute_reply.started": "2024-11-25T00:52:51.120378Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "num_heads = 8\n",
    "d_ff = 2048\n",
    "num_layers = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3732c5",
   "metadata": {},
   "source": [
    "## Model Execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e4254c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:51.125275Z",
     "iopub.status.busy": "2024-11-25T00:52:51.125030Z",
     "iopub.status.idle": "2024-11-25T00:52:51.811350Z",
     "shell.execute_reply": "2024-11-25T00:52:51.810662Z",
     "shell.execute_reply.started": "2024-11-25T00:52:51.125233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    d_ff,\n",
    "    num_layers,\n",
    "    input_vocab_size,\n",
    "    target_vocab_size,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6323faa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:51.812463Z",
     "iopub.status.busy": "2024-11-25T00:52:51.812240Z",
     "iopub.status.idle": "2024-11-25T00:52:51.884197Z",
     "shell.execute_reply": "2024-11-25T00:52:51.883580Z",
     "shell.execute_reply.started": "2024-11-25T00:52:51.812440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "source = source.to(device)\n",
    "target = target.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb96380",
   "metadata": {},
   "source": [
    "Computing the model output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b71ed8be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:51.885247Z",
     "iopub.status.busy": "2024-11-25T00:52:51.885011Z",
     "iopub.status.idle": "2024-11-25T00:52:52.257798Z",
     "shell.execute_reply": "2024-11-25T00:52:52.256937Z",
     "shell.execute_reply.started": "2024-11-25T00:52:51.885223Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouput.shape torch.Size([2, 10, 50])\n"
     ]
    }
   ],
   "source": [
    "output = model(source, target)\n",
    "print(f\"ouput.shape {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4b2910",
   "metadata": {},
   "source": [
    "## Translator Eng-Spa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dff394",
   "metadata": {},
   "source": [
    "### English File Reading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "869a7244",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:52.259651Z",
     "iopub.status.busy": "2024-11-25T00:52:52.258963Z",
     "iopub.status.idle": "2024-11-25T00:52:52.263514Z",
     "shell.execute_reply": "2024-11-25T00:52:52.262659Z",
     "shell.execute_reply.started": "2024-11-25T00:52:52.259578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PATH = \"/kaggle/working/eng-spa4.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d0af1eba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:52.265004Z",
     "iopub.status.busy": "2024-11-25T00:52:52.264503Z",
     "iopub.status.idle": "2024-11-25T00:52:52.737774Z",
     "shell.execute_reply": "2024-11-25T00:52:52.737059Z",
     "shell.execute_reply.started": "2024-11-25T00:52:52.264979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open(PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "eng_spa_pairs = [line.strip().split(\"\\t\") for line in lines if \"\\t\" in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c930226f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:52.739171Z",
     "iopub.status.busy": "2024-11-25T00:52:52.738847Z",
     "iopub.status.idle": "2024-11-25T00:52:52.745404Z",
     "shell.execute_reply": "2024-11-25T00:52:52.744569Z",
     "shell.execute_reply.started": "2024-11-25T00:52:52.739136Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi.', 'Hola.'],\n",
       " ['No.', 'No.'],\n",
       " ['Go.', 'Vaya.'],\n",
       " ['Go!', 'Vete'],\n",
       " ['No!', '¬°No!'],\n",
       " ['Go!', '¬°Fuera!'],\n",
       " ['Go!', '¬°Sal!'],\n",
       " ['Go!', '¬°Ve!'],\n",
       " ['Ah!', '¬°Anda!'],\n",
       " ['Go!', 'V√°yase']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_spa_pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "095f4037",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:52.749921Z",
     "iopub.status.busy": "2024-11-25T00:52:52.749620Z",
     "iopub.status.idle": "2024-11-25T00:52:52.789241Z",
     "shell.execute_reply": "2024-11-25T00:52:52.788651Z",
     "shell.execute_reply.started": "2024-11-25T00:52:52.749871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eng_sentences = [pair[0] for pair in eng_spa_pairs]\n",
    "spa_sentences = [pair[1] for pair in eng_spa_pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0d9e1c95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:52.790908Z",
     "iopub.status.busy": "2024-11-25T00:52:52.790253Z",
     "iopub.status.idle": "2024-11-25T00:52:52.795150Z",
     "shell.execute_reply": "2024-11-25T00:52:52.794273Z",
     "shell.execute_reply.started": "2024-11-25T00:52:52.790863Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi.', 'No.', 'Go.', 'Go!', 'No!', 'Go!', 'Go!', 'Go!', 'Ah!', 'Go!']\n",
      "['Hola.', 'No.', 'Vaya.', 'Vete', '¬°No!', '¬°Fuera!', '¬°Sal!', '¬°Ve!', '¬°Anda!', 'V√°yase']\n"
     ]
    }
   ],
   "source": [
    "print(eng_sentences[:10])\n",
    "print(spa_sentences[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60d11478",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:52.796380Z",
     "iopub.status.busy": "2024-11-25T00:52:52.796116Z",
     "iopub.status.idle": "2024-11-25T00:52:52.812786Z",
     "shell.execute_reply": "2024-11-25T00:52:52.812063Z",
     "shell.execute_reply.started": "2024-11-25T00:52:52.796346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Function to preprocess a sentence by converting to lowercase, removing special characters,\n",
    "    and adding start and end tokens.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed sentence\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"[√°]+\", \"a\", sentence)\n",
    "    sentence = re.sub(r\"[√©]+\", \"e\", sentence)\n",
    "    sentence = re.sub(r\"[√≠]+\", \"i\", sentence)\n",
    "    sentence = re.sub(r\"[√≥]+\", \"o\", sentence)\n",
    "    sentence = re.sub(r\"[√∫]+\", \"u\", sentence)\n",
    "\n",
    "    sentence = re.sub(r\"[^a-z]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    sentence = \"<sos> \" + sentence + \" <eos>\"\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8890975f",
   "metadata": {},
   "source": [
    "### Sample Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "478f673b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:52.813944Z",
     "iopub.status.busy": "2024-11-25T00:52:52.813698Z",
     "iopub.status.idle": "2024-11-25T00:52:52.826076Z",
     "shell.execute_reply": "2024-11-25T00:52:52.825405Z",
     "shell.execute_reply.started": "2024-11-25T00:52:52.813920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "s1 = \"¬øHola @ c√≥mo est√°s? 123\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96ac79c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:52.827137Z",
     "iopub.status.busy": "2024-11-25T00:52:52.826894Z",
     "iopub.status.idle": "2024-11-25T00:52:52.837659Z",
     "shell.execute_reply": "2024-11-25T00:52:52.836805Z",
     "shell.execute_reply.started": "2024-11-25T00:52:52.827098Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬øHola @ c√≥mo est√°s? 123\n",
      "<sos> hola como estas <eos>\n"
     ]
    }
   ],
   "source": [
    "print(s1)\n",
    "print(preprocess_sentence(s1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9fc9c4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:52.839085Z",
     "iopub.status.busy": "2024-11-25T00:52:52.838830Z",
     "iopub.status.idle": "2024-11-25T00:52:59.371020Z",
     "shell.execute_reply": "2024-11-25T00:52:59.370012Z",
     "shell.execute_reply.started": "2024-11-25T00:52:52.839062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eng_sentences = [preprocess_sentence(sentence) for sentence in eng_sentences]\n",
    "spa_sentences = [preprocess_sentence(sentence) for sentence in spa_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7a3b18d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:59.372268Z",
     "iopub.status.busy": "2024-11-25T00:52:59.371995Z",
     "iopub.status.idle": "2024-11-25T00:52:59.377780Z",
     "shell.execute_reply": "2024-11-25T00:52:59.376823Z",
     "shell.execute_reply.started": "2024-11-25T00:52:59.372241Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<sos> hola <eos>',\n",
       " '<sos> no <eos>',\n",
       " '<sos> vaya <eos>',\n",
       " '<sos> vete <eos>',\n",
       " '<sos> no <eos>',\n",
       " '<sos> fuera <eos>',\n",
       " '<sos> sal <eos>',\n",
       " '<sos> ve <eos>',\n",
       " '<sos> anda <eos>',\n",
       " '<sos> vayase <eos>']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spa_sentences[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "97931cd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:59.379089Z",
     "iopub.status.busy": "2024-11-25T00:52:59.378842Z",
     "iopub.status.idle": "2024-11-25T00:52:59.388413Z",
     "shell.execute_reply": "2024-11-25T00:52:59.387652Z",
     "shell.execute_reply.started": "2024-11-25T00:52:59.379066Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences: List[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Function to build a vocabulary from a list of sentences.\n",
    "\n",
    "    Args:\n",
    "        sentences (List[str]): containing input sentences\n",
    "\n",
    "    Returns:\n",
    "        word2idx: dict, mapping words to indices\n",
    "        idx2word: dict, mapping indices to words\n",
    "    \"\"\"\n",
    "\n",
    "    words = [word for sentence in sentences for word in sentence.split()]\n",
    "    word_count = Counter(words)\n",
    "\n",
    "    sorted_word_counts = sorted(word_count.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    word2idx = {word: idx for idx, (word, _) in enumerate(sorted_word_counts, 2)}\n",
    "\n",
    "    word2idx[\"<pad>\"] = 0  # Reserved for padding\n",
    "    word2idx[\"<unk>\"] = 1  # Reserved for unknown words\n",
    "\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c06594",
   "metadata": {},
   "source": [
    "## Building the english & spanish vocabularies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7fa8738e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:52:59.389642Z",
     "iopub.status.busy": "2024-11-25T00:52:59.389408Z",
     "iopub.status.idle": "2024-11-25T00:53:00.531444Z",
     "shell.execute_reply": "2024-11-25T00:53:00.530489Z",
     "shell.execute_reply.started": "2024-11-25T00:52:59.389619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eng_word2idx, eng_idx2word = build_vocab(eng_sentences)\n",
    "spa_word2idx, spa_idx2word = build_vocab(spa_sentences)\n",
    "eng_vocab_size = len(eng_word2idx)\n",
    "spa_vocab_size = len(spa_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "79d6b633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:53:00.532874Z",
     "iopub.status.busy": "2024-11-25T00:53:00.532561Z",
     "iopub.status.idle": "2024-11-25T00:53:00.537168Z",
     "shell.execute_reply": "2024-11-25T00:53:00.536264Z",
     "shell.execute_reply.started": "2024-11-25T00:53:00.532847Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27688 46991\n"
     ]
    }
   ],
   "source": [
    "print(eng_vocab_size, spa_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dcc8786",
   "metadata": {},
   "source": [
    "## EngSpaDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e564017c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:53:00.538786Z",
     "iopub.status.busy": "2024-11-25T00:53:00.538347Z",
     "iopub.status.idle": "2024-11-25T00:53:00.554166Z",
     "shell.execute_reply": "2024-11-25T00:53:00.553368Z",
     "shell.execute_reply.started": "2024-11-25T00:53:00.538749Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EngSpaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    English-Spanish dataset class designed to convert sentences into\n",
    "    word indices using the word-to-index dictionaries.\n",
    "\n",
    "    Attributes:\n",
    "        eng_sentences (List[str]): List of English sentences\n",
    "        spa_sentences (List[str]): List of Spanish sentences\n",
    "        eng_word2idx (dict): Dictionary to map English words to indices\n",
    "        spa_word2idx (dict): Dictionary to map Spanish words to indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        eng_sentences: List[str],\n",
    "        spa_sentences: List[str],\n",
    "        eng_word2idx: dict,\n",
    "        spa_word2idx: dict,\n",
    "    ):\n",
    "        self.eng_sentences = eng_sentences\n",
    "        self.spa_sentences = spa_sentences\n",
    "        self.eng_word2idx = eng_word2idx\n",
    "        self.spa_word2idx = spa_word2idx\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Computes the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: The length of the dataset\n",
    "        \"\"\"\n",
    "\n",
    "        return len(self.eng_sentences)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Gets an item from the dataset at the specified index.\n",
    "\n",
    "        Args:\n",
    "            idx (int): The index of the item to retrieve\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: The English and Spanish sentence indices\n",
    "        \"\"\"\n",
    "\n",
    "        eng_sentence = self.eng_sentences[idx]\n",
    "        spa_sentence = self.spa_sentences[idx]\n",
    "\n",
    "        eng_idxs = [\n",
    "            self.eng_word2idx.get(word, self.eng_word2idx[\"<unk>\"])\n",
    "            for word in eng_sentence.split()\n",
    "        ]\n",
    "\n",
    "        spa_idxs = [\n",
    "            self.spa_word2idx.get(word, self.spa_word2idx[\"<unk>\"])\n",
    "            for word in spa_sentence.split()\n",
    "        ]\n",
    "\n",
    "        return torch.tensor(eng_idxs), torch.tensor(spa_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff13ae",
   "metadata": {},
   "source": [
    "## Collate Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b579577b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:53:00.555369Z",
     "iopub.status.busy": "2024-11-25T00:53:00.555132Z",
     "iopub.status.idle": "2024-11-25T00:53:00.564406Z",
     "shell.execute_reply": "2024-11-25T00:53:00.563645Z",
     "shell.execute_reply.started": "2024-11-25T00:53:00.555346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(\n",
    "    batch: List[Tuple[torch.Tensor, torch.Tensor]]\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Function to pad sequences in a batch to the same length.\n",
    "\n",
    "    Args:\n",
    "        batch (List[Tuple[torch.Tensor, torch.Tensor]]): The batch of data\n",
    "\n",
    "    Returns:\n",
    "        eng_batch: tensor, padded English sentences\n",
    "        spa_batch: tensor, padded Spanish sentences\n",
    "    \"\"\"\n",
    "\n",
    "    eng_batch, spa_batch = zip(*batch)\n",
    "    eng_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in eng_batch]\n",
    "    spa_batch = [seq[:MAX_SEQ_LEN].clone().detach() for seq in spa_batch]\n",
    "\n",
    "    eng_batch = torch.nn.utils.rnn.pad_sequence(\n",
    "        eng_batch, batch_first=True, padding_value=0\n",
    "    )\n",
    "\n",
    "    spa_batch = torch.nn.utils.rnn.pad_sequence(\n",
    "        spa_batch, batch_first=True, padding_value=0\n",
    "    )\n",
    "\n",
    "    return eng_batch, spa_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3708be0",
   "metadata": {},
   "source": [
    "## Train Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d514b7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:53:00.565969Z",
     "iopub.status.busy": "2024-11-25T00:53:00.565463Z",
     "iopub.status.idle": "2024-11-25T00:53:00.577707Z",
     "shell.execute_reply": "2024-11-25T00:53:00.576944Z",
     "shell.execute_reply.started": "2024-11-25T00:53:00.565931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, loss_function, optimiser, epochs):\n",
    "    \"\"\"\n",
    "    Training loop for the Transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (Transformer): The Transformer model\n",
    "        dataloader (DataLoader): The DataLoader object\n",
    "        loss_function (nn.CrossEntropyLoss): The loss function\n",
    "        optimiser (optim.Adam): The optimiser\n",
    "        epochs (int): The number of epochs\n",
    "    \"\"\"\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, (eng_batch, spa_batch) in enumerate(dataloader):\n",
    "            eng_batch = eng_batch.to(device)\n",
    "            spa_batch = spa_batch.to(device)\n",
    "\n",
    "            target_input = spa_batch[:, :-1]\n",
    "            target_output = spa_batch[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            output = model(eng_batch, target_input)\n",
    "            output = output.view(-1, output.size(-1))\n",
    "\n",
    "            loss = loss_function(output, target_output)\n",
    "\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch: {epoch}/{epochs}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb533b",
   "metadata": {},
   "source": [
    "## Data Loader Parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2379ea72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:53:00.578955Z",
     "iopub.status.busy": "2024-11-25T00:53:00.578705Z",
     "iopub.status.idle": "2024-11-25T00:53:00.586996Z",
     "shell.execute_reply": "2024-11-25T00:53:00.586338Z",
     "shell.execute_reply.started": "2024-11-25T00:53:00.578932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee4affae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:53:00.587977Z",
     "iopub.status.busy": "2024-11-25T00:53:00.587719Z",
     "iopub.status.idle": "2024-11-25T00:53:00.595332Z",
     "shell.execute_reply": "2024-11-25T00:53:00.594718Z",
     "shell.execute_reply.started": "2024-11-25T00:53:00.587953Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = EngSpaDataset(eng_sentences, spa_sentences, eng_word2idx, spa_word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "13079c5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:53:00.596407Z",
     "iopub.status.busy": "2024-11-25T00:53:00.596163Z",
     "iopub.status.idle": "2024-11-25T00:53:00.610828Z",
     "shell.execute_reply": "2024-11-25T00:53:00.610012Z",
     "shell.execute_reply.started": "2024-11-25T00:53:00.596370Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f6802e",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e08eef6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:53:00.612031Z",
     "iopub.status.busy": "2024-11-25T00:53:00.611719Z",
     "iopub.status.idle": "2024-11-25T00:53:01.565539Z",
     "shell.execute_reply": "2024-11-25T00:53:01.564384Z",
     "shell.execute_reply.started": "2024-11-25T00:53:00.611997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    d_model=512,\n",
    "    num_heads=8,\n",
    "    d_ff=2048,\n",
    "    num_layers=6,\n",
    "    input_vocab_size=eng_vocab_size,\n",
    "    target_vocab_size=spa_vocab_size,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    dropout=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a1181a12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:53:01.566957Z",
     "iopub.status.busy": "2024-11-25T00:53:01.566699Z",
     "iopub.status.idle": "2024-11-25T00:53:02.530263Z",
     "shell.execute_reply": "2024-11-25T00:53:02.529529Z",
     "shell.execute_reply.started": "2024-11-25T00:53:01.566932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimiser = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "14e265e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T00:53:02.531838Z",
     "iopub.status.busy": "2024-11-25T00:53:02.531325Z",
     "iopub.status.idle": "2024-11-25T02:36:28.469971Z",
     "shell.execute_reply": "2024-11-25T02:36:28.469186Z",
     "shell.execute_reply.started": "2024-11-25T00:53:02.531800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10, Loss: 3.5978\n",
      "Epoch: 1/10, Loss: 2.2009\n",
      "Epoch: 2/10, Loss: 1.7008\n",
      "Epoch: 3/10, Loss: 1.3739\n",
      "Epoch: 4/10, Loss: 1.1242\n",
      "Epoch: 5/10, Loss: 0.9223\n",
      "Epoch: 6/10, Loss: 0.7568\n",
      "Epoch: 7/10, Loss: 0.6298\n",
      "Epoch: 8/10, Loss: 0.5343\n",
      "Epoch: 9/10, Loss: 0.4664\n"
     ]
    }
   ],
   "source": [
    "train(model, dataloader, loss_function, optimiser, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b2bcf",
   "metadata": {},
   "source": [
    "## Auxiliary Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "50740746",
   "metadata": {
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2024-11-25T02:36:28.471494Z",
     "iopub.status.busy": "2024-11-25T02:36:28.471217Z",
     "iopub.status.idle": "2024-11-25T02:36:28.479510Z",
     "shell.execute_reply": "2024-11-25T02:36:28.478659Z",
     "shell.execute_reply.started": "2024-11-25T02:36:28.471468Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence, word2idx):\n",
    "    \"\"\"\n",
    "    Converts a sentence into a list of indices using a word-to-index dictionary.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The sentence to convert.\n",
    "        word2idx (dict): The dictionary mapping words to indices.\n",
    "\n",
    "    Returns:to\n",
    "        list: A list of indices corresponding to the words in the sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    return [word2idx.get(word, word2idx[\"<unk>\"]) for word in sentence.split()]\n",
    "\n",
    "\n",
    "def indices_to_sentence(indices, idx2word):\n",
    "    \"\"\"\n",
    "    Converts a list of indices back into a sentence using an index-to-word dictionary.\n",
    "\n",
    "    Args:\n",
    "        indices (list): The list of indices to convert.\n",
    "        idx2word (dict): The dictionary mapping indices to words.\n",
    "\n",
    "    Returns:\n",
    "        str: The sentence corresponding to the indices.\n",
    "    \"\"\"\n",
    "\n",
    "    return \" \".join(\n",
    "        [\n",
    "            idx2word[idx]\n",
    "            for idx in indices\n",
    "            if idx in idx2word and idx2word[idx] != \"<pad>\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def translate_sentence(\n",
    "    model, sentence, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=\"cpu\"\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Translates a sentence from English to Spanish using the trained Transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained Transformer model.\n",
    "        sentence (str): The English sentence to translate.\n",
    "        eng_word2idx (dict): The dictionary mapping English words to indices.\n",
    "        spa_idx2word (dict): The dictionary mapping Spanish indices to words.\n",
    "        max_len (int): The maximum length of the translated sentence.\n",
    "        device (str): The device to run the model on ('cpu' or 'cuda').\n",
    "\n",
    "    Returns:\n",
    "        str: The translated Spanish sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    input_indices = sentence_to_indices(sentence, eng_word2idx)\n",
    "    input_tensor = torch.tensor(input_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    tgt_indices = [spa_word2idx[\"<sos>\"]]\n",
    "    tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            output = model(input_tensor, tgt_tensor)\n",
    "\n",
    "            output = output.squeeze(0)\n",
    "\n",
    "            next_token = output.argmax(dim=-1)[-1].item()\n",
    "            tgt_indices.append(next_token)\n",
    "            tgt_tensor = torch.tensor(tgt_indices).unsqueeze(0).to(device)\n",
    "\n",
    "            if next_token == spa_word2idx[\"<eos>\"]:\n",
    "                break\n",
    "\n",
    "    return indices_to_sentence(tgt_indices, spa_idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13954aa5",
   "metadata": {},
   "source": [
    "## Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c2c0db72",
   "metadata": {
    "code_folding": [
     15
    ],
    "execution": {
     "iopub.execute_input": "2024-11-25T02:36:28.480904Z",
     "iopub.status.busy": "2024-11-25T02:36:28.480655Z",
     "iopub.status.idle": "2024-11-25T02:36:28.494928Z",
     "shell.execute_reply": "2024-11-25T02:36:28.494188Z",
     "shell.execute_reply.started": "2024-11-25T02:36:28.480881Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_translations(\n",
    "    model, sentences, eng_word2idx, spa_idx2word, max_len=MAX_SEQ_LEN, device=\"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluates translations for a list of sentences using the trained Transformer model.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained Transformer model.\n",
    "        sentences (list): A list of sentences to translate.\n",
    "        eng_word2idx (dict): The dictionary mapping English words to indices.\n",
    "        spa_idx2word (dict): The dictionary mapping Spanish indices to words.\n",
    "        max_len (int): The maximum length of the translated sentence.\n",
    "        device (str): The device to run the model on ('cpu' or 'cuda').\n",
    "    \"\"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        translation = translate_sentence(\n",
    "            model, sentence, eng_word2idx, spa_idx2word, max_len, device\n",
    "        )\n",
    "\n",
    "        print(f\"Input sentence: {sentence}\")\n",
    "        print(f\"Traducci√≥n: {translation}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11f9e36",
   "metadata": {},
   "source": [
    "## Testing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35742176",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T02:37:38.459130Z",
     "iopub.status.busy": "2024-11-25T02:37:38.458774Z",
     "iopub.status.idle": "2024-11-25T02:37:38.463668Z",
     "shell.execute_reply": "2024-11-25T02:37:38.462770Z",
     "shell.execute_reply.started": "2024-11-25T02:37:38.459100Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I am learning artificial intelligence.\",\n",
    "    \"Artificial intelligence is great.\",\n",
    "    \"Good night!\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"Could you please help me find my keys?\",\n",
    "    \"She works at a technology company in Silicon Valley.\",\n",
    "    \"Remember to drink plenty of water throughout the day.\",\n",
    "    \"My favorite season is autumn because of the colorful leaves.\",\n",
    "    \"We should meet for coffee next Tuesday afternoon.\",\n",
    "    \"The new restaurant downtown serves amazing Italian food.\",\n",
    "    \"The children are playing in the park with their friends.\",\n",
    "    \"Don't forget to submit your assignment by Friday.\",\n",
    "    \"The concert last night was absolutely incredible.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5e703f2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T02:36:28.506673Z",
     "iopub.status.busy": "2024-11-25T02:36:28.506338Z",
     "iopub.status.idle": "2024-11-25T02:36:28.527409Z",
     "shell.execute_reply": "2024-11-25T02:36:28.526583Z",
     "shell.execute_reply.started": "2024-11-25T02:36:28.506637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06836633",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-25T02:37:41.168086Z",
     "iopub.status.busy": "2024-11-25T02:37:41.167761Z",
     "iopub.status.idle": "2024-11-25T02:37:42.365368Z",
     "shell.execute_reply": "2024-11-25T02:37:42.364661Z",
     "shell.execute_reply.started": "2024-11-25T02:37:41.168061Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Hello, how are you?\n",
      "Traducci√≥n: <sos> hola como estas <eos>\n",
      "\n",
      "Input sentence: I am learning artificial intelligence.\n",
      "Traducci√≥n: <sos> estoy aprendiendo inteligencia artificial <eos>\n",
      "\n",
      "Input sentence: Artificial intelligence is great.\n",
      "Traducci√≥n: <sos> la inteligencia artificial es muy artificial <eos>\n",
      "\n",
      "Input sentence: Good night!\n",
      "Traducci√≥n: <sos> buenas noches <eos>\n",
      "\n",
      "Input sentence: The weather is beautiful today.\n",
      "Traducci√≥n: <sos> hoy hace un buen tiempo <eos>\n",
      "\n",
      "Input sentence: Could you please help me find my keys?\n",
      "Traducci√≥n: <sos> podrias ayudarme a encontrar mis llaves por favor <eos>\n",
      "\n",
      "Input sentence: She works at a technology company in Silicon Valley.\n",
      "Traducci√≥n: <sos> ella trabaja en una empresa de tecnologia de gravedad <eos>\n",
      "\n",
      "Input sentence: Remember to drink plenty of water throughout the day.\n",
      "Traducci√≥n: <sos> no te olvides de tomar mucha agua por dia <eos>\n",
      "\n",
      "Input sentence: My favorite season is autumn because of the colorful leaves.\n",
      "Traducci√≥n: <sos> mi oto o favorita es la estacion favorita de hojas <eos>\n",
      "\n",
      "Input sentence: We should meet for coffee next Tuesday afternoon.\n",
      "Traducci√≥n: <sos> nos vemos el cafe por la tarde para el proximo martes <eos>\n",
      "\n",
      "Input sentence: The new restaurant downtown serves amazing Italian food.\n",
      "Traducci√≥n: <sos> el nuevo restaurante sirve comida china en el centro de la ciudad de la tierra <eos>\n",
      "\n",
      "Input sentence: The children are playing in the park with their friends.\n",
      "Traducci√≥n: <sos> los ni os estan jugando en el parque con sus amigos <eos>\n",
      "\n",
      "Input sentence: Don't forget to submit your assignment by Friday.\n",
      "Traducci√≥n: <sos> no olvides entregar tu tarea en el viernes <eos>\n",
      "\n",
      "Input sentence: The concert last night was absolutely incredible.\n",
      "Traducci√≥n: <sos> el concierto fue absolutamente increible anoche <eos>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_translations(\n",
    "    model,\n",
    "    test_sentences,\n",
    "    eng_word2idx,\n",
    "    spa_idx2word,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    device=device,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6157876,
     "sourceId": 10003865,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
