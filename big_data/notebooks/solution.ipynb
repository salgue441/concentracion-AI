{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big Data ~ Momento de Retroalimentación\n",
    "\n",
    "### Estudiante\n",
    "\n",
    "- Nombre: Carlos Salguero\n",
    "- Matrícula: A00833341\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The Flickr30k dataset has become a standard benchmark for sentence-based image description. This paper presents Flickr30k Entities, which augments the 158k captions from Flickr30k with 244k coreference chains, linking mentions of the same entities across different captions for the same image, and associating them with 276k manually annotated bounding boxes. Such annotations are essential for continued progress in automatic image description and grounded language understanding. They enable us to define a new benchmark for localization of textual entity mentions in an image. We present a strong baseline for this task that combines an image-text embedding, detectors for common objects, a color classifier, and a bias towards selecting larger objects. While our baseline rivals in accuracy more complex state-of-the-art models, we show that its gains cannot be easily parlayed into improvements on such tasks as image-sentence retrieval, thus underlining the limitations of current methods and the need for further research.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import *\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Caption Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionModel(nn.Module):\n",
    "    def __init__(self, embed_size: int = 256, hidden_size: int = 256):\n",
    "        super(ImageCaptionModel, self).__init__()\n",
    "\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "\n",
    "        self.image_embed = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(embed_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, embed_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)\n",
    "\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.image_embed(features)\n",
    "        features = self.dropout(features)\n",
    "        features = self.projection(features)\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliar Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session(app_name: str = \"Flick39k_PyTorch\") -> SparkSession:\n",
    "    \"\"\"\n",
    "    Creates a spark session with the necessary configuration.\n",
    "    \"\"\"\n",
    "\n",
    "    return (\n",
    "        SparkSession.builder.appName(app_name)\n",
    "        .config(\"spark.driver.memory\", \"16g\")\n",
    "        .config(\"spark.executor.memory\", \"16g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.executor.cores\", \"4\")\n",
    "        .config(\"spark.python.worker.memory\", \"16g\")\n",
    "        .getOrCreate()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path: str, transform=None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Preprocesses the image to be used by the model.\n",
    "    \"\"\"\n",
    "\n",
    "    if transform is None:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    with Image.open(image_path).convert(\"RGB\") as img:\n",
    "        return transform(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining PySpark and PyTorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparkPyTorchTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        spark: SparkSession,\n",
    "        model: nn.Module,\n",
    "        batch_size: int = 32,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        self.spark = spark\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def prepare_data(self, image_dir: str, annotation_file: str):\n",
    "        with open(annotation_file, \"r\") as ann_file:\n",
    "            annotations = json.load(ann_file)\n",
    "\n",
    "        annotations_schema = StructType(\n",
    "            [\n",
    "                StructField(\"image_id\", StringType(), True),\n",
    "                StructField(\"caption\", ArrayType(StringType()), True),\n",
    "                StructField(\n",
    "                    \"boxes\",\n",
    "                    ArrayType(\n",
    "                        StructType(\n",
    "                            [\n",
    "                                StructField(\"x\", FloatType(), True),\n",
    "                                StructField(\"y\", FloatType(), True),\n",
    "                                StructField(\"width\", FloatType(), True),\n",
    "                                StructField(\"height\", FloatType(), True),\n",
    "                            ]\n",
    "                        )\n",
    "                    ),\n",
    "                    True,\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        annotations_df = self.spark.createDataFrame(\n",
    "            annotations, schema=annotations_schema\n",
    "        )\n",
    "\n",
    "        annotations_df = annotations_df.withColumn(\n",
    "            \"image_path\", concat(lit(image_dir), col(\"image_id\"))\n",
    "        )\n",
    "\n",
    "        return annotations_df\n",
    "\n",
    "    def preprocess_batch(self, batch_df):\n",
    "        images, captions = [], []\n",
    "\n",
    "        for row in batch_df.collect():\n",
    "            try:\n",
    "                img_tensor = preprocess_image(row.image_path, self.transform)\n",
    "                images.append(img_tensor)\n",
    "                captions.append(row.caption)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing image: {row.image_path}\")\n",
    "                print(e)\n",
    "\n",
    "                continue\n",
    "\n",
    "        if not images:\n",
    "            return None\n",
    "\n",
    "        image_batch = torch.stack(images).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            image_features = self.model(image_batch)\n",
    "\n",
    "        return image_features, captions\n",
    "\n",
    "    def train(self, data_df, batch_size: int = 32, num_epochs: int = 10):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()\n",
    "            total_loss, num_batches = 0, 0\n",
    "\n",
    "            batches = data_df.repartition(data_df.count() // batch_size)\n",
    "            for batch in batches.toLocalIterator():\n",
    "                batch_df = self.spark.createDataFrame([batch])\n",
    "                result = self.preprocess_batch(batch_df)\n",
    "\n",
    "                if result is None:\n",
    "                    continue\n",
    "\n",
    "                features, captions = result\n",
    "                loss = self.calculate_loss(features, captions)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Epoch: {epoch}, Loss: {avg_loss}\")\n",
    "\n",
    "    def calculate_loss(self, image_features, captions):\n",
    "        return torch.mean(image_features.pow(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.3\n",
      "[('spark.app.name', 'Flick39k_PyTorch'), ('spark.driver.port', '46609'), ('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.app.submitTime', '1729556534865'), ('spark.executor.id', 'driver'), ('spark.app.id', 'local-1729556536176'), ('spark.python.worker.memory', '16g'), ('spark.driver.memory', '16g'), ('spark.driver.host', '10.255.255.254'), ('spark.executor.cores', '4'), ('spark.executor.memory', '16g'), ('spark.app.startTime', '1729556535081'), ('spark.sql.warehouse.dir', 'file:/mnt/d/developer/Tec/concentracion/AI-datascience/big%20data/momento%20de%20retroalimentacion/big%20data/notebooks/spark-warehouse'), ('spark.rdd.compress', 'True'), ('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'), ('spark.serializer.objectStreamReset', '100'), ('spark.driver.maxResultSize', '0'), ('spark.master', 'local[*]'), ('spark.submit.pyFiles', ''), ('spark.submit.deployMode', 'client'), ('spark.ui.showConsoleProgress', 'true')]\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "print(spark.version)\n",
    "print(spark.sparkContext.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carlos/miniconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/carlos/miniconda3/envs/pytorch/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ImageCaptionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SparkPyTorchTrainer(spark, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = trainer.prepare_data(image_dir=\"../data/\", annotation_file=\"../data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(data_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
